{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63eba137",
   "metadata": {},
   "source": [
    "## Mendeley datasets download via API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b23a55",
   "metadata": {},
   "source": [
    "Get your Mendeley API token by registering an app here:\n",
    "\n",
    "https://dev.mendeley.com/myapps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45702a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env from c:\\Users\\david\\***\\apis\\.env\n",
      "CLIENT_SECRET: hF8**********r63\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import os\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from IPython.display import clear_output\n",
    "\n",
    "ROOT = \"..\" # Adjust to repository root\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "DOTENV_PATH = os.path.join(ROOT,\"../../apis/.env\") # Adjust to .env file location\n",
    "\n",
    "if load_dotenv(DOTENV_PATH):\n",
    "    abs_path = os.path.abspath(DOTENV_PATH)\n",
    "    drive, rel = os.path.splitdrive(abs_path)\n",
    "    parts = rel.strip(os.sep).split(os.sep)\n",
    "    masked = parts.copy()\n",
    "    masked_parts = 1  # adjust to mask parts of your path\n",
    "    max_maskable = max(0, len(parts) - 2)\n",
    "    for i in range(min(masked_parts, max_maskable)):\n",
    "        idx = len(parts) - 3 - i\n",
    "        masked[idx] = \"***\"\n",
    "    prefix = f\"{drive}{os.sep}\" if drive else (os.sep if abs_path.startswith(os.sep) else \"\")\n",
    "    print(f\"Loaded .env from {prefix}{os.sep.join(masked)}\")\n",
    "else:\n",
    "    print(\"Failed to load .env file.\")\n",
    "\n",
    "CLIENT_ID = os.getenv(\"MENDELEY_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"MENDELEY_CLIENT_SECRET\")\n",
    "\n",
    "def mask_token(token, unmasked_chars=3):\n",
    "    return token[:unmasked_chars] + '*' * (len(token) - unmasked_chars*2) + token[-unmasked_chars:]\n",
    "print(f\"CLIENT_SECRET: {mask_token(CLIENT_SECRET)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62473c2f",
   "metadata": {},
   "source": [
    "## Worldwide Electricity Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee99e84",
   "metadata": {},
   "source": [
    "## Global Day-Ahead Electricity Price Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc41b0f",
   "metadata": {},
   "source": [
    "Ullah, Md Habib; Reza, Sayed Mohsin; Gundapaneni, Lasya Madhuri; Balachander, Pranav; Babaiahgari, Bhanu; Khan, Abdullah Al Ahad (2025), “Global Day-Ahead Electricity Price Dataset”, Mendeley Data, V3, doi: 10.17632/s54n4tyyz4.3\n",
    "https://data.mendeley.com/datasets/s54n4tyyz4/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47a3170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worldwide Electricity Load Dataset  \n",
      "\n",
      "This dataset provides a comprehensive, global-scale collection of time-series electricity load data aggregated from\n",
      "major power markets across Asia, Europe, North America, and Oceania. It captures system-level demand profiles at\n",
      "consistent temporal resolution, enabling detailed analysis of consumption patterns, peak-demand behavior, and seasonal\n",
      "variability across regions. The dataset is structured in a standardized and harmonized format to support load\n",
      "forecasting, cross-market comparative studies, and data-driven research in power system planning, operation, and energy\n",
      "market analytics.\n",
      "\n",
      "Number_of_files: 489 \n",
      "\n",
      "Additional_Information.csv, Australia_AEMO_2006.csv, Australia_AEMO_2007.csv, Australia_AEMO_2008.csv,\n",
      "Australia_AEMO_2009.csv, Australia_AEMO_2010.csv, Australia_AEMO_2011.csv, Australia_AEMO_2012.csv,\n",
      "Australia_AEMO_2013.csv, Australia_AEMO_2014.csv, Australia_AEMO_2015.csv, Australia_AEMO_2016.csv,\n",
      "Australia_AEMO_2017.csv, Australia_AEMO_2018.csv, Australia_AEMO_2019.csv, Australia_AEMO_2020.csv,\n",
      "Australia_AEMO_2021.csv, Australia_AEMO_2022.csv, Australia_AEMO_2023.csv, Australia_AEMO_2024.csv,\n",
      "Australia_AEMO_2025.csv, Austria_ENTSO-E_2015.csv, Austria_ENTSO-E_2016.csv, Austria_ENTSO-E_2017.csv, Austria_ENTSO-\n",
      "E_2018.csv, Austria_ENTSO-E_2019.csv...\n"
     ]
    }
   ],
   "source": [
    "DATASET_ID = \"ybggkc58fz\"\n",
    "DATASET_VERSION = \"1\"\n",
    "\n",
    "TOKEN_URL = \"https://api.mendeley.com/oauth/token\"\n",
    "DATASET_URL = f\"https://api.mendeley.com/datasets/{DATASET_ID}\"\n",
    "\n",
    "# obtain access token\n",
    "token_resp = requests.post(\n",
    "    TOKEN_URL,\n",
    "    data={\"grant_type\": \"client_credentials\", \"scope\": \"all\"},\n",
    "    auth=HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET),\n",
    ")\n",
    "token_resp.raise_for_status()\n",
    "access_token = token_resp.json()[\"access_token\"]\n",
    "\n",
    "# access dataset metadata\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Accept\": \"application/vnd.mendeley-public-dataset.1+json\",\n",
    "}\n",
    "params = {\"version\": DATASET_VERSION}\n",
    "\n",
    "dataset_resp = requests.get(DATASET_URL, headers=headers, params=params)\n",
    "dataset_resp.raise_for_status()\n",
    "\n",
    "print(dataset_resp.json()[\"name\"], '\\n')\n",
    "print(textwrap.fill(dataset_resp.json()[\"description\"], width=120))\n",
    "\n",
    "data = dataset_resp.json()\n",
    "\n",
    "print(\"\\nNumber_of_files:\", len(data.get(\"files\", [])), \"\\n\")\n",
    "\n",
    "files = data.get(\"files\", [])\n",
    "preview_names = \", \".join(file[\"filename\"] for file in files[:26]) + \"...\"\n",
    "print(textwrap.fill(preview_names, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a828d9e6",
   "metadata": {},
   "source": [
    "Downloading the full dataset via Mendeley API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "639c3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = os.path.join(ROOT, \"datasets\", \"daep_api\")\n",
    "PKL_DIR = os.path.join(ROOT, \"pkl_api\")\n",
    "LOG_PATH = os.path.join(ROOT, \"datasets\", \"daep_api\", \"0_download_errors.log\")\n",
    "\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(PKL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98b1eaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csv_name</th>\n",
       "      <th>record_count</th>\n",
       "      <th>pkl_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Additional_Information.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>Additional_Information.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia_AEMO_2006.csv</td>\n",
       "      <td>17519</td>\n",
       "      <td>Australia_AEMO_2006.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia_AEMO_2007.csv</td>\n",
       "      <td>17520</td>\n",
       "      <td>Australia_AEMO_2007.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia_AEMO_2008.csv</td>\n",
       "      <td>17568</td>\n",
       "      <td>Australia_AEMO_2008.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia_AEMO_2009.csv</td>\n",
       "      <td>17520</td>\n",
       "      <td>Australia_AEMO_2009.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>USA_SPP_2021.csv</td>\n",
       "      <td>8758</td>\n",
       "      <td>USA_SPP_2021.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>USA_SPP_2022.csv</td>\n",
       "      <td>8749</td>\n",
       "      <td>USA_SPP_2022.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>USA_SPP_2023.csv</td>\n",
       "      <td>8757</td>\n",
       "      <td>USA_SPP_2023.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>USA_SPP_2024.csv</td>\n",
       "      <td>8782</td>\n",
       "      <td>USA_SPP_2024.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>USA_SPP_2025.csv</td>\n",
       "      <td>7351</td>\n",
       "      <td>USA_SPP_2025.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>489 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       csv_name  record_count                    pkl_name\n",
       "0    Additional_Information.csv            42  Additional_Information.pkl\n",
       "1       Australia_AEMO_2006.csv         17519     Australia_AEMO_2006.pkl\n",
       "2       Australia_AEMO_2007.csv         17520     Australia_AEMO_2007.pkl\n",
       "3       Australia_AEMO_2008.csv         17568     Australia_AEMO_2008.pkl\n",
       "4       Australia_AEMO_2009.csv         17520     Australia_AEMO_2009.pkl\n",
       "..                          ...           ...                         ...\n",
       "484            USA_SPP_2021.csv          8758            USA_SPP_2021.pkl\n",
       "485            USA_SPP_2022.csv          8749            USA_SPP_2022.pkl\n",
       "486            USA_SPP_2023.csv          8757            USA_SPP_2023.pkl\n",
       "487            USA_SPP_2024.csv          8782            USA_SPP_2024.pkl\n",
       "488            USA_SPP_2025.csv          7351            USA_SPP_2025.pkl\n",
       "\n",
       "[489 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file list to ..\\pkl_api\\0_daep_files_list.pkl\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "files = data.get(\"files\", [])\n",
    "\n",
    "for f in files:\n",
    "    file_name = f.get(\"filename\")\n",
    "    content = f.get(\"content_details\", {})\n",
    "\n",
    "    if not file_name or content.get(\"content_type\") != \"text/csv\":\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(DATASET_DIR, file_name)\n",
    "    pickle_file = os.path.join(PKL_DIR, file_name.replace(\".csv\", \".pkl\"))\n",
    "\n",
    "    try:\n",
    "        # skip download if CSV already exists\n",
    "        if not os.path.exists(file_path):\n",
    "            download_url = content.get(\"download_url\")\n",
    "            if not download_url:\n",
    "                continue\n",
    "\n",
    "            r = requests.get(download_url, stream=True, timeout=60)\n",
    "            r.raise_for_status()\n",
    "\n",
    "            with open(file_path, \"wb\") as fh:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        fh.write(chunk)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        record_count = len(df)\n",
    "\n",
    "        results.append({\"csv_name\": file_name, \"record_count\": record_count})\n",
    "        print(f\"Processed {file_name} with {record_count} records.\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if \"Timestamp\" in df.columns:\n",
    "            df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "\n",
    "        os.makedirs(os.path.dirname(pickle_file), exist_ok=True)\n",
    "        df.to_pickle(pickle_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"{file_name} | {type(e).__name__} | {e}\\n\")\n",
    "        continue\n",
    "\n",
    "csvs_df = pd.DataFrame(results)\n",
    "csvs_df[\"pkl_name\"] = csvs_df[\"csv_name\"].str.replace(\".csv\", \".pkl\")\n",
    "\n",
    "display(csvs_df)\n",
    "\n",
    "list_file_name = \"0_daep_files_list.pkl\"\n",
    "csvs_df.to_pickle(os.path.join(PKL_DIR, list_file_name))\n",
    "print(f\"Saved file list to {os.path.join(PKL_DIR, list_file_name)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
